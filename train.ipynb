{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 02:32:11.571377: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from dimenet.model.dimenet import DimeNet\n",
    "from dimenet.model.dimenet_pp import DimeNetPP\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.metrics import Metrics\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.yaml for DimeNet, config_pp.yaml for DimeNet++\n",
    "with open('config.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "if model_name == \"dimenet\":\n",
    "    num_bilinear = config['num_bilinear']\n",
    "elif model_name == \"dimenet++\":\n",
    "    out_emb_size = config['out_emb_size']\n",
    "    int_emb_size = config['int_emb_size']\n",
    "    basis_emb_size = config['basis_emb_size']\n",
    "    extensive = config['extensive']\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model name: '{model_name}'\")\n",
    "    \n",
    "emb_size = config['emb_size']\n",
    "num_blocks = config['num_blocks']\n",
    "\n",
    "num_spherical = config['num_spherical']\n",
    "num_radial = config['num_radial']\n",
    "output_init = config['output_init']\n",
    "\n",
    "cutoff = config['cutoff']\n",
    "envelope_exponent = config['envelope_exponent']\n",
    "\n",
    "num_before_skip = config['num_before_skip']\n",
    "num_after_skip = config['num_after_skip']\n",
    "num_dense_output = config['num_dense_output']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "targets = config['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 02:32:15 (INFO): Directory: ./20230320_023215_NPZy54xb_md17_aspirin.npz_E_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(targets)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create summary writer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "train = {}\n",
    "validation = {}\n",
    "\n",
    "train['metrics'] = Metrics('train', targets)\n",
    "validation['metrics'] = Metrics('val', targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211762, 21, 3)\n",
      "211762\n",
      "211762\n"
     ]
    }
   ],
   "source": [
    "data_container = DataContainer(dataset, cutoff=cutoff, target_keys=targets)\n",
    "\n",
    "# Initialize DataProvider (splits dataset into 3 sets based on data_seed and provides tf.datasets)\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size,\n",
    "                             seed=data_seed, randomized=True)\n",
    "\n",
    "# Initialize datasets\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"dimenet\":\n",
    "    model = DimeNet(\n",
    "            emb_size=emb_size, num_blocks=num_blocks, num_bilinear=num_bilinear,\n",
    "            num_spherical=num_spherical, num_radial=num_radial,\n",
    "            cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "            num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "            num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "            activation=swish, output_init=output_init)\n",
    "elif model_name == \"dimenet++\":\n",
    "    model = DimeNetPP(\n",
    "            emb_size=emb_size, out_emb_size=out_emb_size,\n",
    "            int_emb_size=int_emb_size, basis_emb_size=basis_emb_size,\n",
    "            num_blocks=num_blocks, num_spherical=num_spherical, num_radial=num_radial,\n",
    "            cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "            num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "            num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "            activation=swish, extensive=extensive, output_init=output_init)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model name: '{model_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load best recorded loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/Documents/DimenetTest/dimenet/dimenet/training/metrics.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(np.log(self.maes)).item()\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps,\n",
    "                  decay_steps, decay_rate,\n",
    "                  ema_decay=ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up checkpointing and load latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Note that the warning `UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.` is expected. It is due to the backward pass of `tf.gather` ([this one](https://github.com/klicperajo/dimenet/blob/master/dimenet/model/layers/interaction_block.py#L62), to be exact) producing sparse gradients, which the previous layer has to convert to a dense tensor (see [this Stack Overflow question](https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fd6df441400>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_1:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_5:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_4:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_8:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_7:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_11:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_10:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_14:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_13:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_17:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_16:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "s\n",
      "(672, 3)\n",
      "(32,)\n",
      "s\n",
      "(672, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 02:32:39.174985: W tensorflow/core/framework/op_kernel.cc:1768] INVALID_ARGUMENT: TypeError: `generator` yielded an element of shape (32, 1, 1) where an element of shape (None, 1) was expected.\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1078, in generator_py_func\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: `generator` yielded an element of shape (32, 1, 1) where an element of shape (None, 1) was expected.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nTypeError: `generator` yielded an element of shape (32, 1, 1) where an element of shape (None, 1) was expected.\nTraceback (most recent call last):\n\n  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1078, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (32, 1, 1) where an element of shape (None, 1) was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_on_batch_22966]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m y \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(x)\n\u001b[0;32m---> 17\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_on_batch(x, y)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Save progress\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m%\u001b[39m save_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nTypeError: `generator` yielded an element of shape (32, 1, 1) where an element of shape (None, 1) was expected.\nTraceback (most recent call last):\n\n  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1078, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (32, 1, 1) where an element of shape (None, 1) was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_on_batch_22966]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 02:32:39.531666: W tensorflow/core/framework/op_kernel.cc:1768] INVALID_ARGUMENT: TypeError: `generator` yielded an element of shape (32, 1, 1) where an element of shape (None, 1) was expected.\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/Users/benni/opt/anaconda3/envs/ml_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1078, in generator_py_func\n",
      "    raise TypeError(\n",
      "\n",
      "TypeError: `generator` yielded an element of shape (32, 1, 1) where an element of shape (None, 1) was expected.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        x = train['dataset_iter']\n",
    "        y = train['metrics']\n",
    "        print(x)\n",
    "        trainer.train_on_batch(x, y)\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                print(i)\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "\n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch+1}): \"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f}; \"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\")\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
